# MLMF ONNX Export - Simplified Format
# Producer: MLMF-ONNX-Export
# Architecture: Custom
# Opset: 17

## Model Graph
Graph: transformer_model

### Inputs
Input: input_ids [Some(1), None] Int64

### Outputs
Output: logits [Some(1), None, Some(50257)] Float32

### Initializers
Weight: model.layers.31.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.21.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.29.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.5.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.23.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.14.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.31.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.14.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.14.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.12.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.8.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.22.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.8.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.7.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.10.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.24.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.6.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.24.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.2.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.30.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.5.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.10.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.11.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.19.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.21.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.9.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.22.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.15.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.29.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.20.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.17.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.31.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.22.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.23.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.14.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.25.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.6.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.11.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.2.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.11.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.16.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.25.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.25.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.3.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.5.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.5.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.16.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.5.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.30.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.25.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.0.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.26.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.3.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.28.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.4.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.17.self_attn.o_proj.weight [4096, 4096]
Weight: model.embed_tokens.weight [32000, 4096]
Weight: model.layers.27.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.13.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.20.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.20.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.27.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.30.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.0.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.10.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.14.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.8.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.9.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.23.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.27.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.21.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.30.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.18.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.8.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.18.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.16.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.8.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.19.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.22.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.23.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.26.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.25.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.19.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.6.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.13.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.16.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.30.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.2.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.14.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.4.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.27.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.24.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.27.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.15.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.7.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.12.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.18.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.17.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.18.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.4.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.27.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.13.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.24.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.17.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.0.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.22.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.29.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.17.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.4.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.20.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.7.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.19.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.12.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.3.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.9.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.19.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.12.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.12.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.1.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.28.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.26.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.26.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.4.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.11.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.29.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.15.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.1.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.15.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.8.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.0.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.9.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.23.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.18.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.11.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.11.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.16.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.1.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.28.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.6.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.14.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.21.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.19.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.20.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.7.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.29.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.16.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.12.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.6.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.28.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.31.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.0.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.11.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.28.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.30.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.6.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.24.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.10.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.13.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.13.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.10.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.0.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.21.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.25.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.31.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.9.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.28.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.22.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.31.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.21.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.7.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.24.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.3.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.30.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.26.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.27.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.2.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.5.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.5.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.28.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.1.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.4.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.10.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.1.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.3.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.29.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.12.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.29.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.3.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.21.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.16.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.7.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.3.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.26.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.18.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.18.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.9.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.13.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.2.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.19.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.20.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.23.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.8.self_attn.o_proj.weight [4096, 4096]
Weight: lm_head.weight [32000, 4096]
Weight: model.layers.13.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.26.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.15.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.15.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.2.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.20.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.1.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.7.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.4.self_attn.q_proj.weight [4096, 4096]
Weight: model.layers.24.self_attn.k_proj.weight [4096, 4096]
Weight: model.layers.6.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.15.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.9.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.23.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.22.self_attn.o_proj.weight [4096, 4096]
Weight: model.layers.25.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.10.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.2.mlp.gate_proj.weight [11008, 4096]
Weight: model.layers.17.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.1.mlp.down_proj.weight [4096, 11008]
Weight: model.layers.31.mlp.up_proj.weight [11008, 4096]
Weight: model.layers.0.self_attn.v_proj.weight [4096, 4096]
Weight: model.layers.17.self_attn.k_proj.weight [4096, 4096]

### Nodes
Node: embedding_lookup [Gather] ["model.embed_tokens.weight", "input_ids"] -> ["embeddings"]
Node: layer_0_attention_matmul [MatMul] ["embeddings", "model.layers.0.self_attn.q_proj.weight"] -> ["layer_0_attention_output"]
Node: layer_0_mlp_matmul [MatMul] ["layer_0_attention_output", "model.layers.0.mlp.down_proj.weight"] -> ["layer_0_mlp_intermediate"]
Node: layer_0_mlp_activation [Relu] ["layer_0_mlp_intermediate"] -> ["layer_0_output"]
Node: layer_1_attention_matmul [MatMul] ["layer_0_output", "model.layers.1.self_attn.q_proj.weight"] -> ["layer_1_attention_output"]
Node: layer_1_mlp_matmul [MatMul] ["layer_1_attention_output", "model.layers.1.mlp.down_proj.weight"] -> ["layer_1_mlp_intermediate"]
Node: layer_1_mlp_activation [Relu] ["layer_1_mlp_intermediate"] -> ["layer_1_output"]
Node: layer_2_attention_matmul [MatMul] ["layer_1_output", "model.layers.2.self_attn.q_proj.weight"] -> ["layer_2_attention_output"]
Node: layer_2_mlp_matmul [MatMul] ["layer_2_attention_output", "model.layers.2.mlp.down_proj.weight"] -> ["layer_2_mlp_intermediate"]
Node: layer_2_mlp_activation [Relu] ["layer_2_mlp_intermediate"] -> ["layer_2_output"]
Node: layer_3_attention_matmul [MatMul] ["layer_2_output", "model.layers.3.self_attn.q_proj.weight"] -> ["layer_3_attention_output"]
Node: layer_3_mlp_matmul [MatMul] ["layer_3_attention_output", "model.layers.3.mlp.gate_proj.weight"] -> ["layer_3_mlp_intermediate"]
Node: layer_3_mlp_activation [Relu] ["layer_3_mlp_intermediate"] -> ["layer_3_output"]
Node: layer_4_attention_matmul [MatMul] ["layer_3_output", "model.layers.4.self_attn.q_proj.weight"] -> ["layer_4_attention_output"]
Node: layer_4_mlp_matmul [MatMul] ["layer_4_attention_output", "model.layers.4.mlp.down_proj.weight"] -> ["layer_4_mlp_intermediate"]
Node: layer_4_mlp_activation [Relu] ["layer_4_mlp_intermediate"] -> ["layer_4_output"]
Node: layer_5_attention_matmul [MatMul] ["layer_4_output", "model.layers.5.self_attn.q_proj.weight"] -> ["layer_5_attention_output"]
Node: layer_5_mlp_matmul [MatMul] ["layer_5_attention_output", "model.layers.5.mlp.gate_proj.weight"] -> ["layer_5_mlp_intermediate"]
Node: layer_5_mlp_activation [Relu] ["layer_5_mlp_intermediate"] -> ["layer_5_output"]
Node: layer_6_attention_matmul [MatMul] ["layer_5_output", "model.layers.6.self_attn.q_proj.weight"] -> ["layer_6_attention_output"]
Node: layer_6_mlp_matmul [MatMul] ["layer_6_attention_output", "model.layers.6.mlp.gate_proj.weight"] -> ["layer_6_mlp_intermediate"]
Node: layer_6_mlp_activation [Relu] ["layer_6_mlp_intermediate"] -> ["layer_6_output"]
Node: layer_7_attention_matmul [MatMul] ["layer_6_output", "model.layers.7.self_attn.q_proj.weight"] -> ["layer_7_attention_output"]
Node: layer_7_mlp_matmul [MatMul] ["layer_7_attention_output", "model.layers.7.mlp.down_proj.weight"] -> ["layer_7_mlp_intermediate"]
Node: layer_7_mlp_activation [Relu] ["layer_7_mlp_intermediate"] -> ["layer_7_output"]
Node: layer_8_attention_matmul [MatMul] ["layer_7_output", "model.layers.8.self_attn.q_proj.weight"] -> ["layer_8_attention_output"]
Node: layer_8_mlp_matmul [MatMul] ["layer_8_attention_output", "model.layers.8.mlp.down_proj.weight"] -> ["layer_8_mlp_intermediate"]
Node: layer_8_mlp_activation [Relu] ["layer_8_mlp_intermediate"] -> ["layer_8_output"]
Node: layer_9_attention_matmul [MatMul] ["layer_8_output", "model.layers.9.self_attn.q_proj.weight"] -> ["layer_9_attention_output"]
Node: layer_9_mlp_matmul [MatMul] ["layer_9_attention_output", "model.layers.9.mlp.down_proj.weight"] -> ["layer_9_mlp_intermediate"]
Node: layer_9_mlp_activation [Relu] ["layer_9_mlp_intermediate"] -> ["layer_9_output"]
Node: layer_10_attention_matmul [MatMul] ["layer_9_output", "model.layers.10.self_attn.q_proj.weight"] -> ["layer_10_attention_output"]
Node: layer_10_mlp_matmul [MatMul] ["layer_10_attention_output", "model.layers.10.mlp.down_proj.weight"] -> ["layer_10_mlp_intermediate"]
Node: layer_10_mlp_activation [Relu] ["layer_10_mlp_intermediate"] -> ["layer_10_output"]
Node: layer_11_attention_matmul [MatMul] ["layer_10_output", "model.layers.11.self_attn.q_proj.weight"] -> ["layer_11_attention_output"]
Node: layer_11_mlp_matmul [MatMul] ["layer_11_attention_output", "model.layers.11.mlp.gate_proj.weight"] -> ["layer_11_mlp_intermediate"]
Node: layer_11_mlp_activation [Relu] ["layer_11_mlp_intermediate"] -> ["layer_11_output"]
Node: layer_12_attention_matmul [MatMul] ["layer_11_output", "model.layers.12.self_attn.q_proj.weight"] -> ["layer_12_attention_output"]
Node: layer_12_mlp_matmul [MatMul] ["layer_12_attention_output", "model.layers.12.mlp.gate_proj.weight"] -> ["layer_12_mlp_intermediate"]
Node: layer_12_mlp_activation [Relu] ["layer_12_mlp_intermediate"] -> ["layer_12_output"]
Node: layer_13_attention_matmul [MatMul] ["layer_12_output", "model.layers.13.self_attn.q_proj.weight"] -> ["layer_13_attention_output"]
Node: layer_13_mlp_matmul [MatMul] ["layer_13_attention_output", "model.layers.13.mlp.up_proj.weight"] -> ["layer_13_mlp_intermediate"]
Node: layer_13_mlp_activation [Relu] ["layer_13_mlp_intermediate"] -> ["layer_13_output"]
Node: layer_14_attention_matmul [MatMul] ["layer_13_output", "model.layers.14.self_attn.q_proj.weight"] -> ["layer_14_attention_output"]
Node: layer_14_mlp_matmul [MatMul] ["layer_14_attention_output", "model.layers.14.mlp.down_proj.weight"] -> ["layer_14_mlp_intermediate"]
Node: layer_14_mlp_activation [Relu] ["layer_14_mlp_intermediate"] -> ["layer_14_output"]
Node: layer_15_attention_matmul [MatMul] ["layer_14_output", "model.layers.15.self_attn.q_proj.weight"] -> ["layer_15_attention_output"]
Node: layer_15_mlp_matmul [MatMul] ["layer_15_attention_output", "model.layers.15.mlp.gate_proj.weight"] -> ["layer_15_mlp_intermediate"]
Node: layer_15_mlp_activation [Relu] ["layer_15_mlp_intermediate"] -> ["layer_15_output"]
Node: layer_16_attention_matmul [MatMul] ["layer_15_output", "model.layers.16.self_attn.q_proj.weight"] -> ["layer_16_attention_output"]
Node: layer_16_mlp_matmul [MatMul] ["layer_16_attention_output", "model.layers.16.mlp.up_proj.weight"] -> ["layer_16_mlp_intermediate"]
Node: layer_16_mlp_activation [Relu] ["layer_16_mlp_intermediate"] -> ["layer_16_output"]
Node: layer_17_attention_matmul [MatMul] ["layer_16_output", "model.layers.17.self_attn.q_proj.weight"] -> ["layer_17_attention_output"]
Node: layer_17_mlp_matmul [MatMul] ["layer_17_attention_output", "model.layers.17.mlp.up_proj.weight"] -> ["layer_17_mlp_intermediate"]
Node: layer_17_mlp_activation [Relu] ["layer_17_mlp_intermediate"] -> ["layer_17_output"]
Node: layer_18_attention_matmul [MatMul] ["layer_17_output", "model.layers.18.self_attn.q_proj.weight"] -> ["layer_18_attention_output"]
Node: layer_18_mlp_matmul [MatMul] ["layer_18_attention_output", "model.layers.18.mlp.gate_proj.weight"] -> ["layer_18_mlp_intermediate"]
Node: layer_18_mlp_activation [Relu] ["layer_18_mlp_intermediate"] -> ["layer_18_output"]
Node: layer_19_attention_matmul [MatMul] ["layer_18_output", "model.layers.19.self_attn.q_proj.weight"] -> ["layer_19_attention_output"]
Node: layer_19_mlp_matmul [MatMul] ["layer_19_attention_output", "model.layers.19.mlp.up_proj.weight"] -> ["layer_19_mlp_intermediate"]
Node: layer_19_mlp_activation [Relu] ["layer_19_mlp_intermediate"] -> ["layer_19_output"]
Node: layer_20_attention_matmul [MatMul] ["layer_19_output", "model.layers.20.self_attn.q_proj.weight"] -> ["layer_20_attention_output"]
Node: layer_20_mlp_matmul [MatMul] ["layer_20_attention_output", "model.layers.20.mlp.up_proj.weight"] -> ["layer_20_mlp_intermediate"]
Node: layer_20_mlp_activation [Relu] ["layer_20_mlp_intermediate"] -> ["layer_20_output"]
Node: layer_21_attention_matmul [MatMul] ["layer_20_output", "model.layers.21.self_attn.q_proj.weight"] -> ["layer_21_attention_output"]
Node: layer_21_mlp_matmul [MatMul] ["layer_21_attention_output", "model.layers.21.mlp.down_proj.weight"] -> ["layer_21_mlp_intermediate"]
Node: layer_21_mlp_activation [Relu] ["layer_21_mlp_intermediate"] -> ["layer_21_output"]
Node: layer_22_attention_matmul [MatMul] ["layer_21_output", "model.layers.22.self_attn.q_proj.weight"] -> ["layer_22_attention_output"]
Node: layer_22_mlp_matmul [MatMul] ["layer_22_attention_output", "model.layers.22.mlp.gate_proj.weight"] -> ["layer_22_mlp_intermediate"]
Node: layer_22_mlp_activation [Relu] ["layer_22_mlp_intermediate"] -> ["layer_22_output"]
Node: layer_23_attention_matmul [MatMul] ["layer_22_output", "model.layers.23.self_attn.q_proj.weight"] -> ["layer_23_attention_output"]
Node: layer_23_mlp_matmul [MatMul] ["layer_23_attention_output", "model.layers.23.mlp.up_proj.weight"] -> ["layer_23_mlp_intermediate"]
Node: layer_23_mlp_activation [Relu] ["layer_23_mlp_intermediate"] -> ["layer_23_output"]
Node: layer_24_attention_matmul [MatMul] ["layer_23_output", "model.layers.24.self_attn.q_proj.weight"] -> ["layer_24_attention_output"]
Node: layer_24_mlp_matmul [MatMul] ["layer_24_attention_output", "model.layers.24.mlp.gate_proj.weight"] -> ["layer_24_mlp_intermediate"]
Node: layer_24_mlp_activation [Relu] ["layer_24_mlp_intermediate"] -> ["layer_24_output"]
Node: layer_25_attention_matmul [MatMul] ["layer_24_output", "model.layers.25.self_attn.q_proj.weight"] -> ["layer_25_attention_output"]
Node: layer_25_mlp_matmul [MatMul] ["layer_25_attention_output", "model.layers.25.mlp.down_proj.weight"] -> ["layer_25_mlp_intermediate"]
Node: layer_25_mlp_activation [Relu] ["layer_25_mlp_intermediate"] -> ["layer_25_output"]
Node: layer_26_attention_matmul [MatMul] ["layer_25_output", "model.layers.26.self_attn.q_proj.weight"] -> ["layer_26_attention_output"]
Node: layer_26_mlp_matmul [MatMul] ["layer_26_attention_output", "model.layers.26.mlp.down_proj.weight"] -> ["layer_26_mlp_intermediate"]
Node: layer_26_mlp_activation [Relu] ["layer_26_mlp_intermediate"] -> ["layer_26_output"]
Node: layer_27_attention_matmul [MatMul] ["layer_26_output", "model.layers.27.self_attn.q_proj.weight"] -> ["layer_27_attention_output"]
Node: layer_27_mlp_matmul [MatMul] ["layer_27_attention_output", "model.layers.27.mlp.down_proj.weight"] -> ["layer_27_mlp_intermediate"]
Node: layer_27_mlp_activation [Relu] ["layer_27_mlp_intermediate"] -> ["layer_27_output"]
Node: layer_28_attention_matmul [MatMul] ["layer_27_output", "model.layers.28.self_attn.q_proj.weight"] -> ["layer_28_attention_output"]
Node: layer_28_mlp_matmul [MatMul] ["layer_28_attention_output", "model.layers.28.mlp.down_proj.weight"] -> ["layer_28_mlp_intermediate"]
Node: layer_28_mlp_activation [Relu] ["layer_28_mlp_intermediate"] -> ["layer_28_output"]
Node: layer_29_attention_matmul [MatMul] ["layer_28_output", "model.layers.29.self_attn.q_proj.weight"] -> ["layer_29_attention_output"]
Node: layer_29_mlp_matmul [MatMul] ["layer_29_attention_output", "model.layers.29.mlp.up_proj.weight"] -> ["layer_29_mlp_intermediate"]
Node: layer_29_mlp_activation [Relu] ["layer_29_mlp_intermediate"] -> ["layer_29_output"]
Node: layer_30_attention_matmul [MatMul] ["layer_29_output", "model.layers.30.self_attn.q_proj.weight"] -> ["layer_30_attention_output"]
Node: layer_30_mlp_matmul [MatMul] ["layer_30_attention_output", "model.layers.30.mlp.up_proj.weight"] -> ["layer_30_mlp_intermediate"]
Node: layer_30_mlp_activation [Relu] ["layer_30_mlp_intermediate"] -> ["layer_30_output"]
Node: layer_31_attention_matmul [MatMul] ["layer_30_output", "model.layers.31.self_attn.q_proj.weight"] -> ["layer_31_attention_output"]
Node: layer_31_mlp_matmul [MatMul] ["layer_31_attention_output", "model.layers.31.mlp.down_proj.weight"] -> ["layer_31_mlp_intermediate"]
Node: layer_31_mlp_activation [Relu] ["layer_31_mlp_intermediate"] -> ["layer_31_output"]
Node: output_projection [MatMul] ["layer_31_output", "lm_head.weight"] -> ["logits"]

# Note: This is a simplified ONNX representation.
# For production use, implement full ONNX protobuf serialization.
